{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-러닝 기초: 미로에서 길 찾기\n",
    "\n",
    "- `E`에서 `G`에 도착하는 가장 빠른 길 찾기\n",
    "\n",
    "```\n",
    "--------------------\n",
    "| A     B    C | D  \n",
    "-----               \n",
    "  E |   F  | G   H |\n",
    "|          ----    |\n",
    "| I     J    K   L |\n",
    "-----------    -----\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 구성\n",
    "\n",
    "- 상태, 행동, 보상 정의\n",
    "\n",
    "1. 모델의 입력 정의\n",
    "\n",
    "2. 수행될 수 있는 행동(=모델의 출력) 정의\n",
    "\n",
    "3. 보상 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 상태\n",
    "\n",
    "- 에이전트는 `A`~`L` 위치 중 임의의 한 곳에 있을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6,\n",
    "    'H': 7,\n",
    "    'I': 8,\n",
    "    'J': 9,\n",
    "    'K': 10,\n",
    "    'L': 11,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 행동\n",
    "\n",
    "- 에이전트는 임의 위치 `X`에서 다른 위치 `Y`로 이동할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = [i for i in range(len(env))]\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 보상\n",
    "\n",
    "- 에이전트는 행동을 취할 때 벽을 넘을 수 없음\n",
    "\n",
    "  - 벽을 넘는 경우 보상에 0을 부여\n",
    "\n",
    "- 특정 위치에서 다른 위치로 이동하는 모든 경우에 대한 보상 행렬을 작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 1000, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       " [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = [[0] * len(env) for _ in range(len(env))]\n",
    "\n",
    "rewards[env['A']][env['B']] = 1\n",
    "\n",
    "rewards[env['B']][env['A']] = 1\n",
    "rewards[env['B']][env['C']] = 1\n",
    "rewards[env['B']][env['F']] = 1\n",
    "\n",
    "rewards[env['C']][env['B']] = 1\n",
    "rewards[env['C']][env['G']] = 1\n",
    "\n",
    "rewards[env['D']][env['H']] = 1\n",
    "\n",
    "rewards[env['E']][env['I']] = 1\n",
    "\n",
    "rewards[env['F']][env['B']] = 1\n",
    "rewards[env['F']][env['J']] = 1\n",
    "\n",
    "rewards[env['G']][env['C']] = 1\n",
    "rewards[env['G']][env['H']] = 1\n",
    "\n",
    "rewards[env['H']][env['D']] = 1\n",
    "rewards[env['H']][env['G']] = 1\n",
    "rewards[env['H']][env['L']] = 1\n",
    "\n",
    "rewards[env['I']][env['E']] = 1\n",
    "rewards[env['I']][env['J']] = 1\n",
    "\n",
    "rewards[env['J']][env['I']] = 1\n",
    "rewards[env['J']][env['F']] = 1\n",
    "rewards[env['J']][env['K']] = 1\n",
    "\n",
    "rewards[env['K']][env['J']] = 1\n",
    "rewards[env['K']][env['L']] = 1\n",
    "\n",
    "rewards[env['L']][env['K']] = 1\n",
    "rewards[env['L']][env['H']] = 1\n",
    "\n",
    "# 최종 목적지에 가장 높은 보상 부여\n",
    "rewards[env['G']][env['G']] = 1000\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 구성\n",
    "\n",
    "### 2-1. 개념 정리\n",
    "\n",
    "#### 2-1-1. Q-value\n",
    "\n",
    "> 특정 상태에서 특정 행동을 취했을 때 에이전트가 기대할 수 있는 미래 보상의 총합\n",
    ">\n",
    "> $Q:(s \\in S,  a \\in A) ➝ Q(s, a) \\in R$\n",
    "\n",
    "$Q(s, a)$는 상태 $s$에서 행동 $a$를 수행할 때의 $Q-value$\n",
    "\n",
    "#### 2-1-2. 시간 차(time difference)\n",
    "\n",
    "> 현재 행동의 결과로 얻은 보상과, 미래 행동의 Q값 간의 차이\n",
    ">\n",
    "> $TD(s_t, a_t) = R(s_t, a_t) = \\gamma \\max(Q(s_{t+1}, a)) - Q(s_t, a_t)$\n",
    "\n",
    "- $Q-value$의 의미를 이해하기 위해 필요한 개념\n",
    "\n",
    "\n",
    "\n",
    "$TD(s_t, a_t)$는 상태 $s_t$에서 행동 $a_t$를 수행함으로써 얻는 보상 $R(s_t, a_t)$에, 미래 상태 $s_{t+1}$에서 수행된 최고 행동의 $Q-value$에 할인 계수 $\\gamma \\in [0, 1]$에 의해 할인된 값을 더한 값\n",
    "\n",
    "- 모델은 더 나은 보상을 얻으려 하므로 시간차가 가장 큰 경우를 찾으려 할 것임\n",
    "\n",
    "- 모델이 받는 보상이 크면 특정 $Q(s, a)$ 값이 증가\n",
    "\n",
    "- 모델은 보상뿐만이 아니라 높은 Q값을 추구해야 함 - $\\gamma \\max(Q(s_{t+1}, a))$\n",
    "\n",
    "- 어떤 시점에서 모델은 높은 Q값으로 이어지는 모든 전이를 알게 될 것이며, 이 전이들의 Q값은 시간이 지남에 따라 이미 증가\n",
    "\n",
    "  - 결국 시간차는 감소하며, 최종 목표에 가까워질수록 시간차는 줄어듦\n",
    "\n",
    "➡️ 즉 모델은 높은 Q값을 추구하며 최종적으로 낮은 시간차를 추구함\n",
    "\n",
    "#### 2-1-3. 벨만 방정식\n",
    "\n",
    "> Q값을 업데이트하는 방법\n",
    "> \n",
    "> $Q_t(s_t, a_t) = Q_{t-1}(s_t, a_t) + \\alpha TD_t(s_t, a_t)$\n",
    "\n",
    "- $\\alpha \\in [0, 1]$은 학습률\n",
    "\n",
    "\n",
    "#### 2-1-4. Q러닝의 목표\n",
    "\n",
    "- 더이상 업데이트되지 않을 때까지 특정 반복 횟수에 걸쳐 Q값을 업데이트하는 것\n",
    "\n",
    "### 2-2. 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "alpha = 0.75\n",
    "\n",
    "state = 4\n",
    "reward = 0\n",
    "\n",
    "q_table = [[0 for _ in range(len(actions))] for _ in range(len(env))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    possible_actions = []\n",
    "    for i in range(len(rewards[state])):\n",
    "        if rewards[state][i] > 0:\n",
    "            possible_actions.append(i)\n",
    "\n",
    "    next_state = np.random.choice(possible_actions)\n",
    "    next_reward = rewards[state][next_state]\n",
    "\n",
    "    time_diff = next_reward + gamma * q_table[next_state][np.argmax(q_table[next_state])] - q_table[state][next_state]\n",
    "\n",
    "    q_table[state][next_state] = q_table[state][next_state] + alpha * time_diff\n",
    "\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E > I > J > F > B > C > G"
     ]
    }
   ],
   "source": [
    "nodes = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L']\n",
    "\n",
    "current = env['E']\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    counter += 1\n",
    "\n",
    "    if counter > 20:\n",
    "        print(\"...\")\n",
    "        break\n",
    "\n",
    "    print(nodes[current], end=\"\")\n",
    "\n",
    "    if current == env['G']:\n",
    "        break\n",
    "\n",
    "    next_ = np.argmax(q_table[current])\n",
    "\n",
    "    if(np.max(q_table[current])) == 0:\n",
    "        break\n",
    "\n",
    "    current = next_\n",
    "\n",
    "    print(' > ', end=\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bansim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
